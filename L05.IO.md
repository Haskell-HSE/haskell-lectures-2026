# I/O, randomness and properties

```haskell
{-# LANGUAGE ImportQualifiedPost #-}
{-# LANGUAGE LambdaCase #-}

module L05_IO where

import Control.Monad (replicateM)
import Data.Foldable (find)

import Control.Selective (ifS)

import System.Random.Stateful (Uniform, globalStdGen, uniformM)

import Test.Falsify.Generator qualified as Gen
import Test.Falsify.Predicate qualified as P
import Test.Falsify.Property (Property, assert, gen)
import Test.Falsify.Range qualified as Range
```

Please note: the following lecture depends on libraries `falsify` and
`selective` which are not present in default installation. For portability, it's
defined as a part of `L05` Cabal project. To check the file, run
`cabal build L05`; to run GHCi, use `cabal repl` option.

## Interfacing with the world

As we mentioned in previous lecture, side-effects in Haskell are modeled using
monads, each one implementing a certain side-effect and all of them adhering to
the `Monad` typeclass, for which a magical `do`-notation is available. But how
to actually interact with the world outside of Haskell process?

Enter `IO`:

```haskell
main :: IO ()
main = do
  putStr "Please tell me your name: "
  name <- getLine
  putStrLn ("Hello, " <> name <> "!")
```

All of the stuff you might possibly want to do with the outside world and/or
with runtime internals is encapsulated using the `IO` monad:

- [terminal input-output, file IO](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/System-IO.html)
- [CLI arguments](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/System-Environment.html)
- [foreign function interface](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/Foreign.html)
- [mutable references](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/Data-IORef.html)
- [exception handling](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/Control-Exception.html)
- [synchronization primitives](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/Control-Concurrent.html)
- [GC tuning](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/System-Mem.html)
- ...and many more!

Generally, it is expected for you to use safe APIs provided by these modules.
"Safe" here means that threading operations through IO provides sequencing of
operations even in presence of laziness, which allows reaching coherence and
basic memory safety. This doesn't save you from unhandled runtime errors and
deadlocks, though.

### IO monad definition

`IO` monad is defined in a
[`System.IO` module](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/System-IO.html).
If you open its definition, you'll see something like

```haskell ignore
newtype IO a = IO (State# RealWorld -> (# State# RealWorld, a #))
```

Ignoring the hashes (they basically mean that data is stored unpacked, without
extra GC references), this is basically a state monad with state of type
`State# RealWorld`. Values of this type are never really used though :D -- it's
just a type-level marker as an additional safety layer. Using it, we can mark
really unsafe builtin operations which are implemented by the Haskell runtime,
e.g.

```haskell ignore
writeWord8OffAddrAsWideChar# :: Addr# -> Int# -> Char# -> State# s -> State# s
```

### Escape hatches

If you _really_ need to run IO action in pure context, you can use
[`unsafePerformIO`](https://hackage-content.haskell.org/package/base-4.22.0.0/docs/System-IO-Unsafe.html#v:unsafePerformIO).
But if you do so, please PLEASE respect the invariants which this function
expects to be true. In most cases, you don't really need this.

There's also this
[little funny beast](https://hackage.haskell.org/package/bytestring/docs/Data-ByteString-Internal.html#v:accursedUnutterablePerformIO).
You've been warned!

## Randomness

In essence, pseudorandom generator is just a function from old generator state
to the new one, yielding a word as a result:

```haskell ignore
genWord :: GenState -> (Int, GenState)
```

In practice, there are many more problems, but most important are the following:

* How to avoid locking on a generator in multithreaded context?
* How to handle state ergonomically, e.g. be able to both use system source of
  randomness in IO and run pure computations in `State` monad with same
  "random-generation" code?
* How to generate arbitrary data from (sequence of) `Int`s?

To help with solution of these problems, there's a
[`random`](https://hackage.haskell.org/package/random) package. We won't really
go into details of how it works, but here's a cheatsheet to help you wade
through type signatures:

* [`RandomGen`](https://hackage-content.haskell.org/package/random-1.3.1/docs/System-Random.html#t:RandomGen)
  is a class of pure generator states;
* [`SplitGen`](https://hackage-content.haskell.org/package/random-1.3.1/docs/System-Random.html#t:SplitGen)
  is a subclass of `RandomGen` for generators which support splitting, solving
  issue with multithreading;
* [`StdGen`](https://hackage-content.haskell.org/package/random-1.3.1/docs/System-Random-Stateful.html#t:StdGen)
  is a default implementation of generator classes, using `splitmix` algorithm;
* [`StatefulGen g m`](https://hackage-content.haskell.org/package/random-1.3.1/docs/System-Random-Stateful.html#t:StatefulGen)
  is a class of monads `m` which support notion of "generator handles" of type
  `g` &mdash; handle isn't a state itself, it just points to where state is
  stored; state updates are performed inside `m`, solving ergonomics issue;
* [`FrozenGen f m`](https://hackage-content.haskell.org/package/random-1.3.1/docs/System-Random-Stateful.html#t:FrozenGen)
  and
  [`ThawedGen f m`](https://hackage-content.haskell.org/package/random-1.3.1/docs/System-Random-Stateful.html#t:ThawedGen)
  link together pure and monadic interface, providing functions to get and set
  pure part `f` of the mutable generator referenced by handle `MutableGen f m`;
* [`Uniform`](https://hackage-content.haskell.org/package/random-1.3.1/docs/System-Random.html#t:Uniform)
  is a class of types which support a notion of "uniform distribution". It is
  implemented for plethora of standard "finite" types, providing deriving for
  custom datatypes;
* [`UniformRange`](https://hackage-content.haskell.org/package/random-1.3.1/docs/System-Random.html#t:UniformRange)
  is a class of types which support a notion of "ranges", and each range
  supports a notion of "uniform distribution". It is implemented for plethora of
  both finite and infinite types, providing deriving for custom datatypes.

The most important functions are:

```haskell ignore
mkStdGen64 :: Word64 -> StdGen
splitGen :: SplitGen g => g -> (g, g)
initStdGen :: MonadIO m => m StdGen

uniform :: (Uniform a, RandomGen g) => g -> (a, g)
uniformR :: (UniformRange a, RandomGen g) => (a, a) -> g -> (a, g)
uniforms :: (Uniform a, RandomGen g) => g -> [a]
uniformRs :: (UniformRange a, RandomGen g) => (a, a) -> g -> [a]

withMutableGen :: ThawedGen f m => f -> (MutableGen f m -> m a) -> m (a, f)

uniformM :: StatefulGen g m => g -> m a
uniformRM :: StatefulGen g m => (a, a) -> g -> m a
```

There are a bunch of deprecated functions and classes here and there, so try to
consult a recent version of documentation when you're not sure.

## Examples

For examples of using IO and pure interface of `random`, consult
[seminar files of introductory course on FP](https://github.com/edashkov/hse-fp-22),
in particular ones called `hse-fp-s4*.hs`.

## Testing for properties

> ...program testing can be a very effective way to show the presence of bugs,
> but is hopelessly inadequate for showing their absence.
<p align="right">&mdash; "The Humble Programmer", Edsger W. Dijkstra</p>

How do you make sure that your program behaves the way you intended it to? I'm
sure you've heard of unit-testing and integration testing. Medium-sized programs
can be carefully reviewed line-by-line. Maybe some of you noticed that types are
a sort-of lightweight specification of a function which is checked by compiler.
Time to add another instrument to your toolbox: property-based testing.

Property is any predicate on (parts of) your program and its inputs.
For example, this one tests whether `reverse . reverse` preserves a particular
input:

```haskell
propRevRev :: Eq a => [a] -> Bool
propRevRev xs = reverse (reverse xs) == xs
```

How do we figure out if this property is universally true? Unit testing
basically runs such property tests on particular inputs. An approach that works
a little bit better is to unit-test on edge cases, and then run on some random
inputs to get some confidence that it works "in general". This is called random
testing; however, it has a little problem.
What to do if property actually fails?

```haskell
propSquareId :: (Eq a, Num a) => a -> Bool
propSquareId x = x * x == x

randomCounterExample :: Uniform a => Int -> (a -> Bool) -> IO (Maybe a)
randomCounterExample count prop = do
  tests <- replicateM count (uniformM globalStdGen)
  pure $ find (not . prop) tests
```

When I ran this in GHCi, I've got something like this:

```
haskell-26/haskell-lectures-2026 $ cabal repl L05
Build profile: -w ghc-9.12.2 -O1
In order, the following will be built (use -v for more details):
 - L05-0.1 (interactive) (lib) (file L05_IO.lhs changed)
Preprocessing library for L05-0.1...
GHCi, version 9.12.2: https://www.haskell.org/ghc/  :? for help
Loaded GHCi configuration from /home/turtle/haskell-26/haskell-lectures-2026/.ghci
[1 of 1] Compiling L05_IO           ( L05_IO.lhs, interpreted )
Ok, one module loaded.
ghci> randomCounterExample 1000 (propSquareId :: Int -> Bool)
Just 6729583473163388195
ghci> randomCounterExample 1000 (propSquareId :: Int -> Bool)
Just 3098392448290831684
ghci> randomCounterExample 1000 (propSquareId :: Int -> Bool)
Just 1789968942886262749
```

Indeed, these are counterexamples to the property that square of a number is
equal to itself. But how do we make sense of it? In case property (and function
which behaviour we're testing) are more complex, such huge counterexamples
wouldn't help a programmer to find a reason why property failed. In other words,
these are _bad_ counterexamples; which are _good_ ones?

Common assumption is that _good_ examples are _the simplest possible_: if `2` is
already a counterexample, why show a number like `3098392448290831684`? Thus
common procedure in _property based testing_ goes like this:

1. Try a bunch of random inputs to see if property fails;
2. If none of the checks failed, show that all tests passed;
3. In case on of the checks failed, try inputs "smaller" than the offending one
   and see if they fail;
4. Continue until the "smallest" counterexample is found.

Therefore, for PBT to work, inputs should not only be randomly generated (and in
a good way &mdash; a counterexample to the property must theoretically be
reachable with good probability), but also _shrinkable_: there should be a
function `shrink :: a -> [a]` which, given a counterexample, produces "smaller"
ones in some sense. For numbers, they should be smaller in absolute value; for
compound structures, sub-structures of a whole should do
(subtrees, subgraphs etc.).

Following this basic idea, in 1999 there appeared the
[`QuickCheck`](https://hackage.haskell.org/package/QuickCheck) library. From
there on, it was successfully remade in C, C++, Clojure, F#, Go, Java,
JavaScript, Prolog, PHP, Python, Rocq, Rust, Scala, Swift, TypeScript,..

To do property-based testing in Haskell today, we recommend to use
[`falsify`](https://hackage.haskell.org/package/falsify). Particular
improvements in comparison with `QuickCheck` are, among others:

- Integrated, internal shrinking: there's a single `Gen a` type which integrates
  both random generation of values of type `a` and shrinking. At the same time,
  `Gen` is also a `Monad`, and shrinking works across the `>>=`, reaching
  simpler counterexamples.
- Simple generation of random functions.
- Integration with [`tasty`](https://hackage.haskell.org/package/tasty) testing
  framework.

## Inner workings of `falsify`

Consider this `falsify` property test that tries to verify the (obviously false)
property that all elements of all lists of up to 10 binary digits are the same
(we will explain the details below; hopefully the intent is clear):

```haskell
propList :: Property ()
propList = do
  n  <- gen $ Gen.inRange $ Range.between (0, 10)
  xs <- gen $ replicateM n $ Gen.int $ Range.between (0, 1)
  assert $ P.pairwise P.eq P..$ ("xs", xs)
```

we might get a counter-example such as this:

```
failed after 9 shrinks
(xs !! 0) /= (xs !! 1)
xs     : [0,1]
xs !! 0: 0
xs !! 1: 1
```

More interesting than the counter-example itself is how `falsify` _arrived_ at
that counter-example; if we look at the shrink history (`--falsify-verbose`), we
see that the list shrunk as follows:

```
   [1,1,0,1,0,1]
~> [1,1,0]       -- shrink the list length
~> [0,1,0]       -- shrink an element of the list
~> [0,1]         -- shrink the list length again
```

The test runner is able to go back and forth between shrinking the length and
the list, and shrinking elements _in_ the list. That is, it has integrated
shrinking (we do not specify a separate generator and shrinker) which works
across monadic bind. The Python [Hypothesis](https://hypothesis.works/) library
showed the world how to achieve this. As we shall see, however, the details are
quite different.

### Parsing versus generation

Generation of inputs relies on pseudo-random number generators (PRNGs). Thus we
might define the type of generators as

```haskell
data PRNG -- details not important

next :: PRNG -> (Word, PRNG)
next = error "details not important"

newtype Gen0 a = Gen0 (PRNG -> (a, PRNG))
```

This covers generation, but not shrinking. The traditional approach in
`QuickCheck` to shrinking is to pair a generator with a shrinking function,
a function of type

```haskell ignore
shrink :: a -> [a]
```

This works, but it’s not without its problems. The key insight of the
`Hypothesis` library is that instead of shrinking generated values, we instead
shrink the samples produced by the PRNG. Suppose we unfold a PRNG to a stream of
random samples:

```haskell
unfoldLinear :: PRNG -> [Word]
unfoldLinear prng =
  let (s, prng') = next prng
  in s : unfoldLinear prng'
```

Then we can shift our perspective: rather than thinking of generating random
values from a PRNG we instead parse this stream of random samples:

```haskell
newtype Parser a = Parser ([Word] -> (a, [Word]))
```

Instead of having a separate shrinking function, we now simply shrink the list
of samples, and then re-run the parser. This is the `Hypothesis` approach in a
nutshell; parsers of course need to ensure that the produced value shrinks as
the samples are shrunk. For example, here is a very simple (proof of concept)
generator for `Bool`:

```haskell
parseBool :: Parser Bool
parseBool = Parser $ \case
  [] -> error "stream should be infinite"
  (s:ss) -> (
    if s >= maxBound `div` 2 then True else False
    , ss
    )
```

Assuming that the sample is chosen uniformly in the full `Word` range, this
parser will choose uniformly between `True` and `False`; and as the sample is
shrunk towards zero, the boolean will shrink towards `False`.

### Streams versus trees

If you look at the definition of `Gen` in `QuickCheck`, you will see it’s
actually different to the definition we showed above:

```haskell
newtype QcGen a = QcGen (PRNG -> a)
```

Like our definition above, this generator takes a `PRNG` as input, but it does
not return an updated `PRNG`. This might seem confusing: suppose we are
generating two numbers, as in our example above; how do we ensure those two
numbers are generated from different `PRNG`s?

To solve this problem, we will need a `PRNG` that in addition to next, also
provides a way to split the `PRNG` into two new `PRNG`s:

```haskell
split :: PRNG -> (PRNG, PRNG)
split = error "details not important"
```

Then to run two generators, we first split the PRNG:

```haskell
bothQc :: QcGen a -> QcGen b -> QcGen (a, b)
bothQc (QcGen g1) (QcGen g2) = QcGen $ \prng ->
  let (l, r) = split prng
  in (g1 l, g2 r)
```

The advantage of this approach is laziness: we can produce the second value of
type `b` without generating the value of type `a` first. Indeed, if we never
demand the value of a, we will not generate it at all! This is of critical
importance if we have generators for infinite values; for example, it is what
enables `falsify` to generate functions.

### The falsify definition of Gen

If we apply the insight from `Hypothesis` (that is, parse samples rather than
generate using `PRNG`s) to this new setting where splitting `PRNG`s is a
fundamental operation, we arrive at the definition of `Gen` in `falsify`. First,
unfolding a `PRNG` does not give us an infinite stream of samples, but rather an
infinite tree of samples:

```haskell
data STree = STree Word STree STree

unfold :: PRNG -> STree
unfold prng =
  let (s, _) = next  prng
      (l, r) = split prng
  in STree s (unfold l) (unfold r)
```

A generator is then a function that takes a part of a sample tree, parses it,
and produces a value and an updated sample tree:

```haskell
newtype Gen a = Gen (STree -> (a, [STree]))
```

This does not reintroduce dependencies between generators: each generator will
be run against a different subtree, and update only that subtree. For example,
here is how we might run two generators:

```haskell
both :: Gen a -> Gen b -> Gen (a, b)
both (Gen g1) (Gen g2) = Gen $ \(STree s l r) ->
  let (a, ls) = g1 l
      (b, rs) = g2 r
  in ( (a, b)
     ,    [STree s l' r  | l' <- ls]
       ++ [STree s l  r' | r' <- rs]
     )
```

Note that we are focussing on the core concepts here, and are glossing over
various details. In particular, the actual definition in `falsify` has an
additional constructor `Minimal`, which is a finite representation of the
infinite tree that is zero everywhere. This is a key component in making this
work with infinite data structures; see the paper for an in-depth discussion.
Users of the library however generally do not need to be aware of this (indeed,
the sample tree abstraction is not part of the public API).

### Consequences of using sample trees

Arguably all of the key differences between `Hypothesis` and `falsify` stem from
the difference in representation of samples: a linear stream in `Hypothesis` and
an infinite tree in `falsify`. In this section we will discuss two consequences
of this choice.

#### Shrinking the sample tree

First, we need to decide how to shrink a sample tree. In `Hypothesis`, the
sample stream (known as a “choice sequence”) is subjected all kinds of passes
(15 and counting, according to
[Test-Case Reduction via Test-Case Generation: Insights from the Hypothesis Reducer](https://2020.ecoop.org/details/ecoop-2020-papers/13/Test-Case-Reduction-via-Test-Case-Generation-Insights-From-the-Hypothesis-Reducer)),
which shrink the sample stream according to lexicographical ordering;
for example:

```
..¸ x, ..        < .., x', ..       -- shrink an element (x' < x)
.., x, y, z, ..  < .., x, z, ..     -- drop an element from the stream
.., x, y, z, ..  < .., y, z, x, ..  -- sort part of the stream (y < z < x)
```

When we are dealing with infinite sample trees, such a total ordering does not
exist. For example, consider the following two trees:

```
tree1 = STree ..         tree2 = STree ..
         (STree 1 ..)              (STree 2 ..)
         (STree 4 ..)              (STree 3 ..)
```

Sample 1 in `tree1` is less than the corresponding sample 2 in `tree2`, but
sample 4 in `tree1` is greater than the corresponding sample 3 in `tree2`.
Hence, we have neither `tree1 < tree2` nor `tree2 < tree1`: these two trees are
incomparable. Instead, `falsify` works with a partial ordering; instead of the
multitude of shrinking passes of `Hypothesis`, `falsify` has precisely one pass:
shrink an individual sample in the tree.

#### Distributing samples to parsers

When we have a stream of values that we need to use for multiple parsers, we
need to decide which samples go to which parser. In `Hypothesis`, this
essentially happens on a first-come-first-served basis: any samples left unused
by the first parser will be used by the next. As discussed, `falsify` parsers do
not return “samples left unused”. Instead, the sample tree is split each time we
compose parsers, like we did in `both`, shown above. In practice, this happens
primarily when using applicative `<*>` or monadic `>>=`.

#### Predictability

These two differences are rather technical in nature; how do they affect users?
Suppose we have a generator that produces a list and then a number:

```haskell ignore
listThenNum :: Gen ([Bool], Int)
listThenNum = do
    xs <- Gen.list ..
    n  <- Gen.int  ..
    return (xs, n)
```

If we are using a stream of samples, `Hypothesis` style, and then drop a random
sample from that stream, the generator for `int` might suddenly be run against
an entirely different sample; it might increase in value! Similarly, if we run
that `int` generator against the first sample left over by the `list` generator,
and if that `list` generator uses fewer samples as it shrinks, we might also run
`int` against an unrelated sample, and its value might again increase.

This is not necessarily a problem; after all, we can then start to decrease that
new `int` value again. However, that is only possible if the generated value
with the larger `int` is still a counter-example to whatever property is being
tested. If that is not the case, then we might not be able to shrink the list,
and we might end up with a non-minimal counter-example. That can make debugging
more difficult (we haven’t gotten rid of all the “junk”), and it can be
difficult for users to understand why this might not shrink any further; even if
the library offers facilities for showing why shrinking stopped (for example,
showing which shrunk examples were rejected; `verbose` mode in `falsify`), it
can still be quite puzzling why the library is trying to increase a value during
shrinking.

Neither of these problems can arise in `falsify`: it never drops samples at all
(instead, only shrinking individual samples), and since monadic bind splits the
sample tree, we are guaranteed that the behaviour of `int` is entirely
unaffected by the behaviour of list. This makes the shrinking behaviour in
`falsify` more predictable and easier to understand.

### Monadic bind

In the example from the introduction we first generated a list length, and then
the elements of the list:

```haskell ignore
propList :: Property ()
propList = do
  n  <- gen $ Gen.integral $ Range.between (0, 10)
  xs <- gen $ replicateM n $ Gen.int $ Range.between (0, 1)
  assert $ P.pairwise P.eq .$ ("xs", xs)
```

With internal shrinking (how it's done in `falsify`) we can go back and forth
across monadic bind. This is the raison d’être of internal shrinking: it doesn’t
matter that we cannot shrink the two generators independently, because we are
not shrinking generators! Instead, we just shrink the samples that feed into
those generators.

### Selective functors

It is important to understand the limitations of internal shrinking: it is
certaintly not a silver bullet. For example, consider this combinator that takes
two generators, flips a coin (generates a boolean, shrinking towards `True`),
and then executes one of the two generators:

```haskell
choose0 :: Gen.Gen a -> Gen.Gen a -> Gen.Gen a -- Suboptimal definition
choose0 g g' = do
  b <- Gen.bool True
  if b then g else g'
```

This combinator works, but it’s not optimal. Suppose the initial value of `b` is
`False`, and so we use `g'`; and let’s suppose furthermore that we spend some
time shrinking the sample tree using `g'`. Consider what happens if `b` now
shrinks to `True`. When this happens we will now run `g` against the sample tree
as it was left after shrinking with `g'`. Although we can do that, it very much
depends on the specific details of `g` and `g'` whether it’s useful to do it,
and we will certainly lose the predictability we discussed above.

We could try to make the two generators shrink independent from each other by
simply running both of them, and using the boolean only to choose which result
we want. After all, Haskell is lazy, and so this should be fine:

```haskell
choose1 :: Gen.Gen a -> Gen.Gen a -> Gen.Gen a -- Bad definition!
choose1 g g' = do
    x <- g
    y <- g'
    b <- Gen.bool True
    return $ if b then x else y
```

While is is true that generation using this definition of `choose` will work
just fine (and laziness ensures that we will in fact only run whatever generator
is used), this combinator shrinks very poorly. The problem is that if we
generate a value but they don’t use it, the (part of) the sample tree that we
used to produce that value is irrelevant, and so by definition we can always
replace it by the sample tree that is zero everywhere. This means that if we
later want to switch to that generator, we will only be able to do so if the
absolute minimum value that the generator can produce happens to work for
whatever property we’re testing. This is an important lesson to remember:

**Do not generate values and then discard them: such values will always shrink
to their minimum. (Instead, don’t generate the value at all.)**

To solve this problem, we need to make it visible to the library when we need a
generator and when we do not, so that we it can avoid shrinking that part of the
sample tree while the generator is not in use. Selectively omitting effects is
precisely what _selective applicative functors_ give us. A detailed discussion
of this topic would take us well outside the scope of the lecture; in the
remainder of this section we will discuss the basics only.

`Gen` is a selective functor, which means that it is an instance of `Selective`,
which has a single method called `select`:

```haskell ignore
select :: Gen (Either a b) -> Gen (a -> b) -> Gen b
```

The intuition is that we run the first generator; if that produces `Left a`, we
run the second generator to get a `b`; if the first generator produces
`Right b`, we skip the second generator completely. Like for applicative `(<*>)`
and monadic `(>>=)`, the two generators are run against different subtrees of
the sample tree, but the critical difference is that we will not try to shrink
the right subtree for the second generator unless that generator is used.

If that all sounds a bit abstract, perhaps suffices to say that any selective
functor supports

```haskell ignore
ifS :: Selective f => f Bool -> f a -> f a -> f a
```

which we can use to implement `choose` in a way that avoids reusing the sample
tree of the first generator for the second:

```haskell
choose :: Gen.Gen a -> Gen.Gen a -> Gen.Gen a
choose = ifS (Gen.bool True)
```

Indeed, this is precisely the definition in the `falsify` library itself.

## Sources

- [Wikipedia page on QuickCheck](https://en.wikipedia.org/wiki/QuickCheck)
- [Blogpost on shrinking in `falsify`](https://www.well-typed.com/blog/2023/04/falsify/)
- [Paper on `falsify`](https://dl.acm.org/doi/10.1145/3609026.3609733)
